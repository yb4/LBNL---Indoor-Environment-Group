{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import pytz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import gc\n",
    "#% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##########################################################################################################\n",
    "1. FUNCTIONS: General Support Function\n",
    "##########################################################################################################\n",
    "'''\n",
    "def checkLogType(series):\n",
    "    import random\n",
    "    diff = series.diff().dropna()\n",
    "    diff = diff.reset_index(drop=True)\n",
    "    randList = random.sample(range(len(diff)), 3)\n",
    "    if diff[randList].iloc[0] == diff[randList].iloc[1] == diff[randList].iloc[2]:\n",
    "        return 'state'\n",
    "    else:\n",
    "        return 'event'\n",
    "\n",
    "def dtFormatInspector(dt, fmt):\n",
    "    try:\n",
    "        pd.to_datetime(dt, format=fmt)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def dtFormatString(dtcol):\n",
    "    if dtFormatInspector(dtcol[5], '%d/%m/%Y %H:%M') and dtFormatInspector(dtcol[dtcol.index[-5]], '%d/%m/%Y %H:%M'):\n",
    "        fmtString = '%d/%m/%Y %H:%M'\n",
    "    elif dtFormatInspector(dtcol[5], '%m/%d/%Y %H:%M') and dtFormatInspector(dtcol[dtcol.index[-5]], '%m/%d/%Y %H:%M'):\n",
    "        fmtString = '%m/%d/%Y %H:%M'\n",
    "    elif dtFormatInspector(dtcol[5], '%m/%d/%Y %H:%M:%S'):\n",
    "        fmtString = '%m/%d/%Y %H:%M:%S'\n",
    "    elif dtFormatInspector(dtcol[5], '%m/%d/%Y %I:%M %p'):\n",
    "        fmtString = '%m/%d/%Y %I:%M %p'\n",
    "    elif dtFormatInspector(dtcol[5], '%m/%d/%Y %I:%M:%S %p'):\n",
    "        fmtString = '%m/%d/%Y %I:%M:%S %p'       \n",
    "    elif dtFormatInspector(dtcol[5], '%m/%d/%y %H:%M'):\n",
    "        fmtString = '%m/%d/%y %H:%M'\n",
    "    elif dtFormatInspector(dtcol[5], '%m/%d/%y %H:%M:%S'):\n",
    "        fmtString = '%m/%d/%y %H:%M:%S'    \n",
    "    elif dtFormatInspector(dtcol[5], '%m/%d/%y %I:%M %p'):\n",
    "        fmtString = '%m/%d/%y %I:%M %p'\n",
    "    elif dtFormatInspector(dtcol[5], '%m/%d/%y %I:%M:%S %p'):\n",
    "        fmtString = '%m/%d/%y %I:%M:%S %p'    \n",
    "    elif dtFormatInspector(dtcol[5], '%m/%d/%y %H:%M:%S.%f'):\n",
    "        fmtString = '%m/%d/%y %H:%M:%S.%f'      \n",
    "    elif dtFormatInspector(dtcol[5], '%Y-%m-%d %H:%M:%S'):\n",
    "        fmtString = '%Y-%m-%d %H:%M:%S'\n",
    "    elif dtFormatInspector(dtcol[5], '%Y-%m-%d %I:%M:%S %p'):\n",
    "        fmtString = '%Y-%m-%d %I:%M:%S %p' \n",
    "    elif dtFormatInspector(dtcol[5], '%Y/%m/%d %H:%M:%S'):\n",
    "        fmtString = '%Y/%m/%d %H:%M:%S'\n",
    "    elif dtFormatInspector(dtcol[5], '%d-%m-%Y,%H:%M:%S'):\n",
    "        fmtString = '%d-%m-%Y,%H:%M:%S'   \n",
    "    elif dtFormatInspector(dtcol[5], '%d-%m-%Y%H:%M:%S'):\n",
    "        fmtString = '%d-%m-%Y%H:%M:%S'\n",
    "    elif dtFormatInspector(dtcol[5], '%Y-%m-%d%H:%M:%S'):\n",
    "        fmtString = '%Y-%m-%d%H:%M:%S'\n",
    "    elif dtFormatInspector(dtcol[5], '%m-%d-%Y %H:%M'):\n",
    "        fmtstring = '%m-%d-%Y %H:%M'\n",
    "    else:\n",
    "        print('datetime format error')\n",
    "    return fmtString    \n",
    "\n",
    "def extractName(fpath):\n",
    "    if fpath[-4:].lower() == '.csv' or fpath[-4:].lower() == '.xls':\n",
    "        fname = fpath[:-4]\n",
    "    elif fpath[-5:].lower() == '.xlsx':\n",
    "        fname = fpath[:-5]\n",
    "    elif fpath[-4:].lower() == '.txt':\n",
    "        fname = fpath[:-4]\n",
    "    return fname\n",
    "\n",
    "def findHeader(fpath, lookup):\n",
    "    if fpath[-4:] == '.csv':\n",
    "        with open(fpath) as f:\n",
    "            for num, line in enumerate(f, 1):\n",
    "                if lookup in line:\n",
    "                    rowNum = num   \n",
    "    elif fpath[-5:] == '.xlsx':\n",
    "        df = pd.read_excel(fpath, header=None, skiprows=0, index_col=False)\n",
    "        rowNum = df.loc[df.iloc[:,0]==lookup].index[0]  \n",
    "    return rowNum\n",
    "\n",
    "def fromExcelDate(dtcol):\n",
    "    # All credit to StackOverflow for this function:\n",
    "    # https://stackoverflow.com/questions/29387137/...\n",
    "    # ...how-to-convert-a-given-ordinal-number-from-excel-to-a-date\n",
    "    \n",
    "    # dtcol MUST be a series    \n",
    "    epoch0 = pd.to_datetime('1899-12-31')\n",
    "    \n",
    "    # Excel leap year bug, 1900 is not a leap year!  \n",
    "    dtcol = dtcol.astype(float)\n",
    "    dtcol.where(dtcol<=59, dtcol-1, inplace=True)          \n",
    "    td = pd.to_timedelta(dtcol, unit='days')\n",
    "    return (epoch0 + td).dt.round('1s')\n",
    "\n",
    "def temperatureConverter(temp, current_units):\n",
    "    if current_units.lower() == 'f':\n",
    "        # convert from Fahrenheit to celsius\n",
    "        celsius = (temp - 32) * (5/9)\n",
    "        output = celsius\n",
    "    elif current_units.lower() == 'c':\n",
    "        # convert from celsius to fahrenheit\n",
    "        fahrenheit = (temp * (9/5)) + 32\n",
    "        output = fahrenheit\n",
    "    return output\n",
    "\n",
    "def fileInspector(prefix, rawF, suffix):  \n",
    "    is_instrFile = lambda f: f.startswith(prefix) and f.lower().endswith(suffix)\n",
    "    fList = [f for f in listdir(rawF) if isfile(join(rawF,f)) and is_instrFile(f)]\n",
    "    return fList\n",
    "\n",
    "def folderCrawler(rawF, home_id, week=None):\n",
    "    if week == 1:\n",
    "        test = 'ON'\n",
    "#         fmtIBU = '{}-{}-Week{}-{}-'\n",
    "    elif week == 2:\n",
    "        test = 'OFF'\n",
    "#         fmtIBU = '{}-{}-Week{}-{}-'\n",
    "    elif week == None:\n",
    "        test = None\n",
    "#         fmtIBU = '{}-{}-'\n",
    "    instrKeys = ['ANM','AVP','CLR','FRM','IBU','MTR','PLD','PMI','PMO',\n",
    "                 'RAD','RDSTR','STA','TRH','TRO','UFP','WTR','WX']\n",
    "    instrValues = []\n",
    "    formatDict = {i:'{}-{}-' for i in instrKeys}\n",
    "#     formatDict['IBU'] = fmtIBU    \n",
    "#     if home_id.startswith('3'):\n",
    "#         formatDict['PMO'] = '{}-{}-Smp1Min'\n",
    "    for instr in instrKeys:\n",
    "        template = formatDict[instr]       \n",
    "        if instr in set(['AVP','ANM','FRM','MTR','PLD','PMI','PMO','STA','TRH','TRO','RAD','RDSTR','WTR','WX','CLR','UFP']):            \n",
    "            prefix = template.format(home_id, instr)            \n",
    "            suffix = ('.csv','.xls', '.xlsx','.txt')           \n",
    "            fList = fileInspector(prefix, rawF, suffix)\n",
    "            instrValues.append(fList)            \n",
    "        elif instr in set(['IBU']):\n",
    "            if week == 1 or week == 2:\n",
    "                prefix = template.format(home_id, instr, week, test)\n",
    "            elif week == None:\n",
    "                prefix = template.format(home_id, instr)\n",
    "            suffix = ('.csv','.xls','.xlsx')  \n",
    "            fList = fileInspector(prefix, rawF, suffix)\n",
    "            instrValues.append(fList) \n",
    "        del template, suffix        \n",
    "    instrDict = dict(zip(instrKeys, instrValues))\n",
    "    return (instrDict, test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n"
     ]
    }
   ],
   "source": [
    "print('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home 224 ON\n",
      "2019-12-30 18:00:00 to 2019-01-06 09:00:00\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "ANM:\n",
      "  224-ANM-BA2.xls\n",
      "  224-ANM-BA1.xls\n",
      "  224-ANM-KIT.xls\n",
      "  224-ANM-AS1.xls\n",
      "AVP:\n",
      "CLR:\n",
      "FRM:\n",
      "  224-FRM-BR1.csv\n",
      "  224-FRM-IN1.csv\n",
      "IBU:\n",
      "  224-IBU-FP1.csv\n",
      "  224-IBU-COF.csv\n",
      "  224-IBU-OV1.csv\n",
      "  224-IBU-CLF.csv\n",
      "  224-IBU-CLB.csv\n",
      "  224-IBU-TOA.csv\n",
      "  224-IBU-CRB.csv\n",
      "  224-IBU-CRF.csv\n",
      "MTR:\n",
      "  224-MTR-CDR.csv\n",
      "PLD:\n",
      "PMI:\n",
      "  224-PMI-IN1.CSV\n",
      "PMO:\n",
      "RAD:\n",
      "  224-RAD-IN1.csv\n",
      "RDSTR:\n",
      "STA:\n",
      "  224-STA-DGH.csv\n",
      "  224-STA-DP1.csv\n",
      "  224-STA-BR1.csv\n",
      "  224-STA-DFR.csv\n",
      "TRH:\n",
      "  224-TRH-BR1.csv\n",
      "  224-TRH-AS1.csv\n",
      "TRO:\n",
      "  224-TRO-BA2.csv\n",
      "  224-TRO-BA1.csv\n",
      "  224-TRO-OUT.csv\n",
      "UFP:\n",
      "WTR:\n",
      "WX:\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "26 files\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "##########################################################################################################\n",
    "2. Input: General Setup\n",
    "##########################################################################################################\n",
    "'''\n",
    "# Set HomeID and Week#\n",
    "home_id = '224'\n",
    "week = 1\n",
    "\n",
    "##############################################################################################\n",
    "# Specify folders\n",
    "rawF = './Home_' + home_id + '/' # Raw files location\n",
    "tsF = './' # read time period from HomeCharacteristics file\n",
    "outF = './' # DataTable output folder\n",
    "if int(home_id) in range(400,500):\n",
    "    ts_fname = 'HomeCharacteristics_Output_401_425.xlsx'\n",
    "elif int(home_id) in range(200,400):\n",
    "    ts_fname = 'HomeCharacteristics_Output.xlsx' \n",
    "elif int(home_id) in range(500,600):\n",
    "    ts_fname = 'HomeCharacteristics_Output_ICRT.xlsx'\n",
    "    \n",
    "if int(home_id) in set(range(900,1000)): # KVZNE study\n",
    "        startrow = 'Field_StartDate'\n",
    "        stoprow = 'Field_EndDate'\n",
    "elif int(home_id) in set(range(200,600)): # BAIAQ study\n",
    "        if week == 1 or week == None:\n",
    "            startrow = 'Field_StartDate_Week1'\n",
    "            stoprow = 'Field_EndDate_Week1'\n",
    "        elif week == 2:\n",
    "            startrow = 'Field_StartDate_Week2'\n",
    "            stoprow = 'Field_EndDate_Week2'\n",
    "else:\n",
    "    print(\"change home_id\")\n",
    "\n",
    "##############################################################################################\n",
    "# Load/Filter DataFrames \n",
    "# daylight saving not considered, change stophour to enable manual adjustment in datatables\n",
    "ts_df = pd.read_excel(tsF + ts_fname, header=0, index_col=0)\n",
    "\n",
    "start = pd.to_datetime(ts_df.loc[startrow, int(home_id)], format = '%Y-%m-%d') + pd.Timedelta(hours=18)\n",
    "stop = pd.to_datetime(ts_df.loc[stoprow, int(home_id)], format = '%Y-%m-%d') + pd.Timedelta(hours=9)\n",
    "startday = start.strftime('%Y-%m-%d')\n",
    "stopday = stop.strftime('%Y-%m-%d')\n",
    "\n",
    "df_index = pd.DataFrame(index = pd.date_range(start=start, end=stop, freq='T'))\n",
    "dataTable = df_index\n",
    "\n",
    "##############################################################################################\n",
    "# establish data file list\n",
    "instrDict, test = folderCrawler(rawF, home_id, week)\n",
    "\n",
    "##############################################################################################\n",
    "# Print stuff\n",
    "print('Home {} {}'.format(home_id, test))\n",
    "print(str(start) + ' to ' + str(stop))\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "count = 0\n",
    "for x in instrDict:\n",
    "    print (str(x) + ':')\n",
    "    for y in instrDict[x]:\n",
    "        print('  '+ str(y))\n",
    "        count += 1\n",
    "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print(str(count) + ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "##########################################################################################################\n",
    "3. FUNCTIONS: Reading and Processing raw data files\n",
    "##########################################################################################################\n",
    "'''\n",
    "def readFile(f, instr):\n",
    "    if instr == 'AVP':\n",
    "        df = read_AVP(f)\n",
    "    elif instr == 'ANM':\n",
    "        df = read_ANM(f)\n",
    "    elif instr == 'CLR':\n",
    "        df = read_CLR(f)\n",
    "    elif instr == 'FRM':\n",
    "        df = read_FRM(f)\n",
    "    elif instr == 'IBU':\n",
    "        df = read_IBU(f)\n",
    "    elif instr == 'MTR':\n",
    "        df = read_MTR(f)\n",
    "    elif instr == 'PLD':\n",
    "        df = read_PLD(f)\n",
    "    elif instr == 'PMI':\n",
    "        df = read_PMI(f)\n",
    "    elif instr == 'PMO':\n",
    "        df = read_PMO(f)\n",
    "    elif instr == 'STA':\n",
    "        df = read_STA(f)\n",
    "    elif instr == 'TRH' or instr == 'TRO':\n",
    "        df = read_TRH(f)\n",
    "    elif instr == 'UFP':\n",
    "        df = read_UFP(f)\n",
    "    elif instr == 'WTR' or instr == 'WX':\n",
    "        df = read_WTR(f)\n",
    "    elif instr == 'RAD' or instr == 'RDSTR':\n",
    "        df = read_RAD(f)\n",
    "    return df\n",
    "\n",
    "def processFile(f, ind, instr, home_id, week=1):\n",
    "    if instr == 'AVP':\n",
    "        df = process_AVP(f, ind)\n",
    "    elif instr == 'ANM':\n",
    "        df = process_ANM(f, ind)\n",
    "    elif instr == 'FRM':\n",
    "        df = process_FRM(f, ind)\n",
    "    elif instr == 'IBU':\n",
    "        df = process_IBU(f, ind)\n",
    "    elif instr == 'MTR':\n",
    "        df = process_MTR(f, ind)\n",
    "    elif instr == 'PLD':\n",
    "        df = process_PLD(f, ind)\n",
    "    elif instr == 'PMI':\n",
    "        df = process_PMI(f, ind)\n",
    "    elif instr == 'PMO':\n",
    "        df = process_PMO(f, ind)\n",
    "    elif instr == 'STA':\n",
    "        df = process_STA(f, ind)\n",
    "    elif instr == 'TRH' or instr == 'TRO':\n",
    "        df = process_TRH(f, ind)\n",
    "    elif instr == 'RAD' or instr == 'RDSTR':\n",
    "        df = process_RAD(f, ind)\n",
    "    elif instr == 'WTR' or instr == 'WX':\n",
    "        df = process_WTR(f,ind)\n",
    "    elif instr == 'CLR':\n",
    "        df = process_CLR(f,ind)\n",
    "    elif instr == 'UFP':\n",
    "        df = process_UFP(f,ind)\n",
    "    return df\n",
    "\n",
    "##############################################################################################\n",
    "# 3.1 ANM - ANEMOMETER: 30sec data (rolling average)\n",
    "# insert last row with \"NA\" for 901\n",
    "##############################################################################################\n",
    "\n",
    "def read_ANM(ANMfile): # .csv, .xls, .xlsx, insert last row as \"NA\" if endhour earlier than 9am\n",
    "    readParamsDict = {'skiprows': 6,'index_col': False}\n",
    "    header_ANM = ['id', 'velocity_mps', 'unit', 'date', 'time']\n",
    "\n",
    "    if ANMfile.endswith(\"csv\"):    \n",
    "        ANM = pd.read_csv(rawF + ANMfile, names=header_ANM, **readParamsDict)\n",
    "        ANM = ANM.apply(lambda x: x.replace(' ', '', regex=True) if x.dtype == \"object\" else x)\n",
    "    elif ANMfile.endswith(\"xls\"):\n",
    "        ANM = pd.read_csv(rawF + ANMfile, names=header_ANM, sep='\\t|,', engine='python', **readParamsDict)\n",
    "        ANM = ANM.apply(lambda x: x.replace(' ', '', regex=True) if x.dtype == \"object\" else x)\n",
    "        ANM.iloc[:,[3,4]] = ANM.iloc[:,[3,4]].apply(lambda x: x.str.strip('\"'))     \n",
    "    elif ANMfile.endswith(\"xlsx\"):\n",
    "        df = pd.read_excel(rawF + ANMfile, header=None, skiprows=0, index_col=False)\n",
    "        rows = findHeader(rawF + ANMfile, 'ID')\n",
    "        try:\n",
    "            header_ANM = ['id', 'velocity_mps', 'unit', 'datetime']\n",
    "            ANM = pd.read_excel(rawF + ANMfile, names=header_ANM, header=0, skiprows=rows, index_col=False)\n",
    "        except:\n",
    "            header_ANM = ['id', 'velocity_mps', 'unit', 'date', 'time']\n",
    "            ANM = pd.read_excel(rawF + ANMfile, names=header_ANM, header=0, skiprows=rows, index_col=False)       \n",
    "        ANM = ANM.apply(lambda x: x.replace(' ', '', regex=True) if x.dtype == \"object\" else x)\n",
    "    else:\n",
    "        print(\"ANM file format error\")\n",
    "    \n",
    "    if ANM.loc[:,'unit'][0].lower() == 'ft/min':\n",
    "        ANM.loc[:,'velocity_mps'] = ANM.loc[:,'velocity_mps'] * 0.00507999983744\n",
    "        ANM.loc[:,'unit'] = 'm/s'\n",
    "    elif ANM.loc[:,'unit'][0].lower() != 'ft/min' and ANM.loc[:,'unit'][0].lower() != 'm/s':\n",
    "        print(\"ANM unit error\")\n",
    "    \n",
    "    if len(ANM.columns) == 4:\n",
    "        ANM['datetime'] = pd.to_datetime(ANM['datetime'], format=dtFormatString(ANM['datetime']))\n",
    "    else:\n",
    "        fmt = dtFormatString(ANM['date'].astype(str) + ANM['time'].astype(str))\n",
    "        ANM.loc[:,'datetime'] = pd.to_datetime(ANM['date'].astype(str) + ANM['time'].astype(str), format=fmt)\n",
    "    \n",
    "    if ANM['datetime'].iloc[-1] < stop:\n",
    "        newdatetime = ANM['datetime'].iloc[-1]+pd.Timedelta(minutes=1)\n",
    "        newrow = pd.Series({'datetime' : newdatetime , 'velocity_mps' : None})\n",
    "        ANM = ANM.append(newrow,ignore_index = True)\n",
    "    \n",
    "    ANM.index = ANM['datetime']  \n",
    "    ANM = ANM['velocity_mps']    \n",
    "    return ANM\n",
    "\n",
    "\n",
    "def process_ANM(ANMfile, ind):\n",
    "    fname = extractName(ANMfile)    \n",
    "    home_id, instr, loc = fname.split('-',2)\n",
    "    pad = 600 # seconds\n",
    "    rawANM = read_ANM(ANMfile)\n",
    "    rawANM = rawANM[(rawANM.index>start-pd.Timedelta(seconds=pad)) & (rawANM.index<stop+pd.Timedelta(seconds=pad))]\n",
    "    \n",
    "    ANM = pd.merge_asof(ind, rawANM, left_index=True, right_index=True, direction='backward')    \n",
    "    colmapper = {'velocity_mps':'{}_{}'.format(instr, loc)}\n",
    "    ANM.rename(columns = colmapper, inplace=True)\n",
    "    ANM.fillna('NA', inplace=True)\n",
    "    return ANM\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# 3.2 AVP - AIR VISUAL PRO: 10sec-5min data (rolling average)\n",
    "##############################################################################################\n",
    "def read_AVP(AVPfile): # .csv, .xlsx\n",
    "    home_id, instr, loc = re.split('-', AVPfile[:-4])[0:3]\n",
    "    if AVPfile.endswith('xlsx'):        \n",
    "        AVP = pd.read_excel(rawF + AVPfile, header=0, index_col=False)\n",
    "    elif AVPfile.endswith('csv'):\n",
    "        AVP = pd.read_csv(rawF+AVPfile, sep=',')\n",
    "    elif AVPfile.endswith('txt'):\n",
    "        AVP = pd.read_csv(rawF+AVPfile, sep=';')\n",
    "    else:\n",
    "        print('AVP file format error')\n",
    "    \n",
    "    AVP['datetime'] = pd.to_datetime(AVP.Date.astype(str) + ' ' + AVP.Time.astype(str),errors='coerce')\n",
    "    AVP = AVP[AVP['datetime']<stop+pd.Timedelta(days=1)]\n",
    "    AVP = AVP[['datetime','PM2_5(ug/m3)','PM10(ug/m3)','Temperature(C)','Humidity(%RH)','CO2(ppm)']]\n",
    "    AVP.columns = ['datetime','pm25_ugm3','pm10_ugm3','temp_degC','rh_pct','co2_ppm']\n",
    "    AVP = AVP[AVP['pm25_ugm3']!= 1798.8]\n",
    "    AVP = AVP[AVP['pm10_ugm3']!= 2000]\n",
    "    if AVP['datetime'].iloc[-1] < stop:\n",
    "        newdatetime = AVP['datetime'].iloc[-1]+pd.Timedelta(minutes=1)\n",
    "        newrow = pd.Series({'datetime' : newdatetime})\n",
    "        AVP = AVP.append(newrow,ignore_index = True)  \n",
    "    AVP = AVP.drop_duplicates(subset='datetime',keep = 'first')    \n",
    "    AVP.index = AVP['datetime']\n",
    "    AVP = AVP[['pm25_ugm3','pm10_ugm3','temp_degC','rh_pct','co2_ppm']]\n",
    "    return AVP\n",
    "\n",
    "\n",
    "def process_AVP(AVPfile, ind):\n",
    "    fname = extractName(AVPfile)    \n",
    "    home_id, instr, loc = re.split('-', fname)[0:3]\n",
    "    pad = 600 # seconds\n",
    "    rawAVP = read_AVP(AVPfile)\n",
    "    rawAVP = rawAVP[(rawAVP.index>start-pd.Timedelta(seconds=pad)) & (rawAVP.index<stop+pd.Timedelta(seconds=pad))]\n",
    "\n",
    "    rawAVP = rawAVP.dropna()\n",
    "    rawAVP[['pm10_ugm3','rh_pct','co2_ppm']] = rawAVP[['pm10_ugm3','rh_pct','co2_ppm']].astype(np.float)\n",
    "    smAVP = rawAVP.resample('s').interpolate(method='time', axis=0, limit_area='inside')\n",
    "    smAVP.index = smAVP.index.shift(-1, freq='30s')\n",
    "    smAVP = smAVP.rolling('60s').mean()\n",
    "    AVP = pd.merge(ind, smAVP, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    colmapper = {'pm25_ugm3':'{}_{}_PM25'.format(instr, loc),\n",
    "                 'pm10_ugm3':'{}_{}_PM10'.format(instr, loc),\n",
    "                 'co2_ppm':'{}_{}_CO2'.format(instr, loc),\n",
    "                 'temp_degC':'{}_{}_T'.format(instr, loc),\n",
    "                 'rh_pct':'{}_{}_RH'.format(instr, loc)}\n",
    "    \n",
    "    AVP.rename(columns = colmapper, inplace=True)\n",
    "    AVP.fillna('NA', inplace=True)\n",
    "    return AVP\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# 3.3 CLR - Clarity: UTC, 3min data, pm1/2.5/10/no2(all raw),temp,RH,tVoc,eCo2 (interpolation)\n",
    "##############################################################################################\n",
    "def read_CLR(CLRfile):\n",
    "    UtcConvert = 7 # 7 hours for PDT, 8 hours for PST\n",
    "    if CLRfile.endswith('.csv'):\n",
    "        CLR = pd.read_csv(rawF + CLRfile, header = 0,index_col = False)\n",
    "    else:\n",
    "        print('CLR file format error')        \n",
    "    CLR['timeUtc'] = pd.to_datetime(CLR['timeUtc'],format='%Y-%m-%d'+'T'+'%H:%M:%S'+'.000Z')\n",
    "    CLR['datetime'] = CLR['timeUtc'] - pd.Timedelta(hours = UtcConvert)\n",
    "    CLR.index = CLR['datetime']\n",
    "    CLR = CLR[['pm1MassConcRaw[ug/m3]','pm2_5MassConcRaw[ug/m3]','pm10MassConcRaw[ug/m3]','no2ConcRaw[ppb]',\n",
    "               'temperature[degC]','relHumidity[%]','tVocConc[ppb]','eCo2Conc[ppb]']]\n",
    "    CLR.columns = ['pm1_ugm3','pm25_ugm3','pm10_ugm3','no2_ppb','temp_degC','rh_pct',\n",
    "                  'tVoc_ppb','eCO2_ppb']\n",
    "    return CLR\n",
    "\n",
    "\n",
    "def process_CLR(CLRfile,ind):\n",
    "    fname = extractName(CLRfile)    \n",
    "    home_id, instr, loc = re.split('-', fname)[0:3]\n",
    "    \n",
    "    pad = 360\n",
    "    rawCLR = read_CLR(CLRfile)\n",
    "    rawCLR = rawCLR[(rawCLR.index>start-pd.Timedelta(seconds=pad)) & (rawCLR.index<stop+pd.Timedelta(seconds=pad))]\n",
    "    smCLR = rawCLR.resample('s').interpolate(method='time',axis=0, limit_area = 'inside')\n",
    "    smCLR.index = smCLR.index.shift(-1, freq='30s')\n",
    "    smCLR = smCLR.rolling('60s').mean()\n",
    "    CLR = pd.merge(ind,smCLR, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    colmapper = {'pm1_ugm3':'{}_{}_PM1'.format(instr, loc),\n",
    "             'pm25_ugm3':'{}_{}_PM25'.format(instr, loc),\n",
    "             'pm10_ugm3':'{}_{}_PM10'.format(instr, loc),\n",
    "             'no2_ppb':'{}_{}_NO2'.format(instr, loc),\n",
    "             'temp_degC':'{}_{}_T'.format(instr, loc),\n",
    "             'rh_pct':'{}_{}_RH'.format(instr, loc),\n",
    "             'tVoc_ppb':'{}_{}_tVOC'.format(instr, loc),\n",
    "             'eCO2_ppb':'{}_{}_eCO2'.format(instr, loc)}\n",
    "    CLR.rename(columns = colmapper, inplace=True)\n",
    "    CLR.fillna('NA',inplace=True)\n",
    "    return CLR\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# 3.4 FRM - Formaldehyde: 30min data (backfill)\n",
    "##############################################################################################\n",
    "def read_FRM(FRMfile): # .csv, .xlsx\n",
    "    readParamsDict = {'header': 0, 'index_col': False}    \n",
    "    header = ['mode', 'sampnum', 'datanum', \n",
    "              'date', 'time', 'frm_ppb', 'frm_ugm3', \n",
    "              'temp_degC', 'temp_degF', 'rh_pct', 'period_min']\n",
    "    if FRMfile[-4:] == '.csv':  \n",
    "        df = pd.read_csv(rawF + FRMfile, names=header, **readParamsDict)\n",
    "    elif FRMfile[-5:] == '.xlsx':\n",
    "        df = pd.read_excel(rawF + FRMfile, names=header, **readParamsDict)\n",
    "        \n",
    "    #df['date'] = pd.to_datetime(df['date'],format = dtFormatString(df['date']))\n",
    "    df.index = pd.to_datetime(df.date.astype(str) + ' ' + df.time.astype(str))\n",
    "    df.loc[df['frm_ppb']=='<0','frm_ppb'] = 0\n",
    "    df.loc[df['frm_ppb']=='Baseline','frm_ppb'] = None\n",
    "    cols_to_use = ['frm_ppb']\n",
    "    df = df[cols_to_use]\n",
    "    df = df.apply(lambda x: pd.to_numeric(x, errors = 'coerce')).dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_FRM(FRMfile, ind):    \n",
    "    fname = extractName(FRMfile)    \n",
    "    home_id, instr, loc = re.split('-', fname)[0:3]\n",
    "    pad = 40 # minutes\n",
    "    rawFRM = read_FRM(FRMfile)\n",
    "    rawFRM = rawFRM[(rawFRM.index>start-pd.Timedelta(minutes=pad)) & (rawFRM.index<stop+pd.Timedelta(minutes=pad))]\n",
    "    FRM = pd.merge(ind, rawFRM.resample('s').bfill(), left_index=True, right_index=True, how='left')\n",
    "    colmapper = {'frm_ppb':'{}_{}'.format(instr, loc)}\n",
    "    FRM.rename(columns = colmapper, inplace=True)\n",
    "    FRM.fillna('NA', inplace=True)\n",
    "    return FRM\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# 3.5 IBU - iButton T: 1-2min data (interpolate)\n",
    "##############################################################################################\n",
    "def read_IBU(IBUfile): # .csv, .xlsx\n",
    "    if IBUfile.endswith('csv'):\n",
    "        if int(home_id) in range(200,500):\n",
    "            IBU = pd.read_csv(rawF+IBUfile,names = ['datetime','temp_degC'],skiprows=8,index_col=None)\n",
    "            IBU = IBU.iloc[:-1]\n",
    "            IBU['datetime'] = fromExcelDate(IBU['datetime'])    \n",
    "            # IBU['datetime'] = pd.to_datetime(IBU['datetime'], format=dtFormatString(IBU['datetime']))\n",
    "        elif int(home_id) in range(900,1000):\n",
    "            header = ['datetime', 'unit', 'temp_degC']    \n",
    "            rows = findHeader(rawF + IBUfile, 'Value')\n",
    "            IBU = pd.read_csv(rawF + IBUfile, skiprows=rows, header=None, names=header, engine='python', skipfooter=1)  \n",
    "            IBU['datetime'] = pd.to_datetime(IBU['datetime'],format = dtFormatString(IBU['datetime']))\n",
    "            if IBU.loc[:,'unit'][0].lower() != 'c':\n",
    "                print(\"IBU unit error\")\n",
    "    elif IBUfile.endswith('xlsx'):\n",
    "        if int(home_id) in range (200,500):\n",
    "            IBUi = pd.read_excel(rawF + IBUfile, header=None, skiprows=0, index_col=False)\n",
    "            rows = IBUi.loc[IBUi.iloc[:,0]=='Date - Time'].index[0]  \n",
    "            IBU = pd.read_excel(rawF + IBUfile, skiprows=rows, header=0, names=['datetime','temp_degC'])\n",
    "            IBU = IBU[IBU['datetime'].astype(str) != '-end-']\n",
    "            if dtFormatInspector(IBU.loc[:,'datetime'][5], '%m/%d/%y %H:%M:%S.%f'):\n",
    "                IBU['datetime'] = pd.to_datetime(IBU['datetime'], format='%m/%d/%y %H:%M:%S.%f').dt.round('1s')    \n",
    "            elif IBU.loc[:,'datetime'].dtypes == float:\n",
    "                IBU['datetime'] = fromExcelDate(IBU['datetime'])    \n",
    "            else:\n",
    "                IBU['datetime'] = pd.to_datetime(IBU['datetime'], format=dtFormatString(IBU['datetime']))\n",
    "    IBU.index = IBU.datetime \n",
    "    IBU = IBU['temp_degC']\n",
    "    return IBU\n",
    "\n",
    "\n",
    "def process_IBU(IBUfile, ind):\n",
    "    fname = extractName(IBUfile)\n",
    "    home_id, instr, loc = fname.split('-',2)\n",
    "    \n",
    "    pad = 240\n",
    "    rawIBU = read_IBU(IBUfile)\n",
    "    rawIBU = rawIBU[(rawIBU.index>start-pd.Timedelta(seconds=pad)) & (rawIBU.index<stop+pd.Timedelta(seconds=pad))]\n",
    "\n",
    "    IBU = pd.merge(ind, rawIBU, left_index=True, right_index=True, how='outer')\n",
    "    IBU = IBU.interpolate(method='time', axis=0, limit_area='inside')\n",
    "    IBU = pd.merge(ind, IBU, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    colmapper = {'temp_degC':'{}_{}'.format(instr, loc)}\n",
    "    IBU.rename(columns = colmapper, inplace=True)\n",
    "    IBU.fillna('NA', inplace=True)\n",
    "    return IBU\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# 3.6 MTR - Motor on/off state: event record (convert) or 1min state data \n",
    "##############################################################################################\n",
    "def read_MTR(MTRfile): # KVZNE 'event' type record - be careful with duplicates\n",
    "    if MTRfile.endswith('csv'):\n",
    "        MTR = pd.read_csv(rawF+MTRfile, header = None, sep=',',skiprows=findHeader(rawF+MTRfile,'Date')).iloc[:,0:3]\n",
    "        MTR.columns = ['datanum','datetime','event/state']\n",
    "        MTR['datetime'] = pd.to_datetime(MTR['datetime'], format = dtFormatString(MTR['datetime']))\n",
    "        if checkLogType(MTR['datetime']) == 'event':\n",
    "            start = MTR['datetime'].iloc[0]\n",
    "            stop = MTR['datetime'].iloc[-1]\n",
    "            new_idx = pd.DataFrame(index = pd.date_range(start, stop, freq='s'))\n",
    "            MTR = pd.merge(new_idx, MTR, how='left', left_index=True, right_on='datetime').fillna(method='ffill')\n",
    "            MTR.loc[:,'motor_pct_on'] = MTR.loc[:,'event/state'] * 100 \n",
    "            MTR.index = MTR['datetime']\n",
    "            MTR = MTR['motor_pct_on']\n",
    "            print('MTR logtype: event')\n",
    "        elif checkLogType(MTR['datetime']) == 'state':\n",
    "            print('MTR logtype: state')\n",
    "            MTR['motor_pct_on'] = MTR['event/state']\n",
    "            MTR.index = MTR['datetime']\n",
    "            MTR = MTR['motor_pct_on']\n",
    "    else:\n",
    "        print('MTR file format error')\n",
    "    return MTR\n",
    "\n",
    "\n",
    "def process_MTR(MTRfile, ind):\n",
    "    fname = extractName(MTRfile)    \n",
    "    home_id, instr, loc = re.split('-', fname)[0:3]\n",
    "    \n",
    "    pad = 300 # seconds\n",
    "    rawMTR = read_MTR(MTRfile)\n",
    "    rawMTR = rawMTR[(rawMTR.index>start-pd.Timedelta(seconds=pad)) & (rawMTR.index<stop+pd.Timedelta(seconds=pad))]    \n",
    "    MTR = pd.merge(ind, rawMTR, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    colmapper = {'motor_pct_on':'{}_{}'.format(instr, loc)}    \n",
    "    MTR.rename(columns = colmapper, inplace=True)\n",
    "    MTR.fillna('NA', inplace=True)\n",
    "    return MTR\n",
    "\n",
    "##############################################################################################\n",
    "# 3.7 PLD - Plug Load: 1min data\n",
    "##############################################################################################\n",
    "def read_PLD(PLDfile): # KVZNE 'event' type record - be careful with duplicates\n",
    "    if PLDfile.endswith('csv'):\n",
    "        PLD = pd.read_csv(rawF+PLDfile, header = None, sep=',',skiprows=findHeader(rawF+PLDfile,'Logged')).iloc[:,0:3]\n",
    "        PLD.columns = ['datanum','datetime','active_power_W']\n",
    "        PLD['datetime'] = pd.to_datetime(PLD['datetime'], format = dtFormatString(PLD['datetime']))\n",
    "        PLD.index = PLD['datetime']\n",
    "        PLD = PLD['active_power_W']\n",
    "    else:\n",
    "        print('PLD file format error')\n",
    "    return PLD\n",
    "\n",
    "\n",
    "def process_PLD(PLDfile, ind):\n",
    "    fname = extractName(PLDfile)    \n",
    "    home_id, instr, loc = re.split('-', fname)[0:3]\n",
    "    \n",
    "    pad = 120\n",
    "    rawPLD = read_PLD(PLDfile)\n",
    "    rawPLD = rawPLD[(rawPLD.index>start-pd.Timedelta(seconds=pad)) & (rawPLD.index<stop+pd.Timedelta(seconds=pad))]\n",
    "    rawPLD = rawPLD.resample('s').bfill()\n",
    "    PLD = pd.merge(ind, rawPLD, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    colmapper = {'active_power_W':'{}_{}'.format(instr, loc)}\n",
    "    PLD.rename(columns = colmapper, inplace=True)\n",
    "    PLD.fillna('NA', inplace=True)\n",
    "    return PLD\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# 3.8 PMI - Indoor PM2.5: 1-2min data \n",
    "##############################################################################################\n",
    "def read_PMI(PMIfile):\n",
    "    if home_id == '906':\n",
    "        PMIStart = pd.to_datetime('02/12/2019 3:45:57 PM',format = '%m/%d/%Y %I:%M:%S %p')\n",
    "        PMI = pd.read_excel(rawF+PMIfile, skiprows = 24)\n",
    "        PMI.index = pd.to_datetime(PMI['datetime'])\n",
    "        PMI['pm_in_ugm3'] = pd.to_numeric(PMI[' \"ug/m3\"'])\n",
    "        PMI = PMI['pm_in_ugm3']\n",
    "    elif int(home_id) in range(200,400):\n",
    "        PMI = pd.read_csv(rawF+PMIfile, header=0)\n",
    "        PMI.drop_duplicates(subset='Time', keep='first',inplace = True)\n",
    "        PMI['datetime'] = pd.to_datetime(PMI['Time'],format = dtFormatString(PMI['Time']))\n",
    "        PMI['pm_in_ugm3'] = pd.to_numeric(PMI['Conc'])\n",
    "        PMI.index = PMI['datetime']\n",
    "        PMI = PMI['pm_in_ugm3']\n",
    "    elif int(home_id) in range(400,500):\n",
    "        PMI = pd.read_excel(rawF+PMIfile, header=0)\n",
    "        PMI.drop_duplicates(subset='Time', keep='first',inplace = True)\n",
    "        PMI['datetime'] = pd.to_datetime(PMI['Time'],format = dtFormatString(PMI['Time']))\n",
    "        PMI['pm_in_ugm3'] = pd.to_numeric(PMI['Conc'])\n",
    "        PMI.index = PMI['datetime']\n",
    "        PMI = PMI['pm_in_ugm3']\n",
    "    elif int(home_id) in range(900,1000):\n",
    "        IndStartDate = findHeader(rawF+PMIfile,'Test Start Date')\n",
    "        IndStartTime = findHeader(rawF+PMIfile,'Test Start Time')\n",
    "        PMIStartDate = pd.read_csv(rawF + PMIfile, nrows = IndStartDate-1).iloc[-1,1]\n",
    "        PMIStartTime = pd.read_csv(rawF + PMIfile, nrows = IndStartTime-1).iloc[-1,1]\n",
    "        PMIStart = pd.to_datetime(PMIStartDate + ' ' + PMIStartTime, format = '%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "        PMI = pd.read_csv(rawF + PMIfile, header = findHeader(rawF + PMIfile,'Elapsed Time [s]')-2).iloc[:,0:2]\n",
    "        PMI['Elapsed Time [s]'] = pd.to_numeric(PMI['Elapsed Time [s]'])\n",
    "        PMI['Elapsed Time [s]'] = pd.to_timedelta(PMI['Elapsed Time [s]'],unit='s')\n",
    "        PMI['datetime'] = PMIStart + PMI['Elapsed Time [s]']\n",
    "        PMI.index = PMI['datetime']\n",
    "        PMI['pm_in_ugm3'] = pd.to_numeric(PMI['Mass [mg/m3]'])*1000\n",
    "        PMI = PMI['pm_in_ugm3']\n",
    "    return PMI\n",
    "\n",
    "\n",
    "def process_PMI(PMIfile,ind):\n",
    "    fname = extractName(PMIfile)    \n",
    "    home_id, instr, loc = fname.split('-',2)\n",
    "    \n",
    "    pad = 600\n",
    "    rawPMI = read_PMI(PMIfile)\n",
    "    rawPMI = rawPMI[(rawPMI.index>start-pd.Timedelta(seconds=pad)) & (rawPMI.index<stop+pd.Timedelta(seconds=pad))]\n",
    "    \n",
    "#     smPMI = rawPMI.resample('s').interpolate(method='time', axis=0, limit_area='inside')\n",
    "#     smPMI.index = smPMI.index.shift(-1, freq='30s')\n",
    "#     smPMI = smPMI.rolling('60s').mean()\n",
    "    smPMI = rawPMI.resample('s').bfill()\n",
    "    PMI = pd.merge(ind, smPMI, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    colmapper = {'pm_in_ugm3':'PMin'}\n",
    "    PMI.rename(columns = colmapper, inplace=True)\n",
    "    PMI.fillna('NA', inplace=True)\n",
    "    return PMI\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# 3.9 PMO - Outdoor PM2.5: 1-2min data (interpolate)\n",
    "##############################################################################################\n",
    "def read_PMO(PMOfile): \n",
    "    if int(home_id) in range(900,1000):\n",
    "        IndStartDate = findHeader(rawF+PMOfile,'Test Start Date')\n",
    "        IndStartTime = findHeader(rawF+PMOfile,'Test Start Time')\n",
    "        PMOStartDate = pd.read_csv(rawF + PMOfile, nrows = IndStartDate-1).iloc[-1,1]\n",
    "        PMOStartTime = pd.read_csv(rawF + PMOfile, nrows = IndStartTime-1).iloc[-1,1]\n",
    "        PMOStart = pd.to_datetime(PMOStartDate + ' ' + PMOStartTime, format = '%m/%d/%Y %I:%M:%S %p')\n",
    "        PMO = pd.read_csv(rawF + PMOfile, header = findHeader(rawF + PMOfile,'Elapsed Time [s]')-2).iloc[:,0:2]\n",
    "        PMO['Elapsed Time [s]'] = pd.to_numeric(PMO['Elapsed Time [s]'])\n",
    "        PMO['Elapsed Time [s]'] = pd.to_timedelta(PMO['Elapsed Time [s]'],unit='s')\n",
    "        PMO['datetime'] = PMOStart + PMO['Elapsed Time [s]']\n",
    "        PMO.index = PMO['datetime']\n",
    "        PMO['pm_out_ugm3'] = pd.to_numeric(PMO['Mass [mg/m3]'])*1000\n",
    "        PMO = PMO['pm_out_ugm3']\n",
    "    elif int(home_id) in range(200,400): # USE Smp1Min\n",
    "        PMO = pd.read_csv(rawF + PMOfile, skiprows = 4)\n",
    "        PMO.columns = ['datetime','RN','pm_out_ugm3','flowrate','T','RH','BP','Status']\n",
    "        PMO = PMO.loc[PMO['flowrate']!=0,:]  # Do not import data when flowrate = 0 (zeroing values)\n",
    "        PMO = PMO.loc[PMO['flowrate']!='0',:]\n",
    "        PMO['datetime'] = pd.to_datetime(PMO['datetime'],format = dtFormatString(PMO['datetime']))\n",
    "        PMO = PMO.loc[PMO['pm_out_ugm3']!='NAN',:]\n",
    "        PMO['pm_out_ugm3'] = pd.to_numeric(PMO['pm_out_ugm3'])\n",
    "        PMO.index = PMO['datetime']\n",
    "        PMO = PMO['pm_out_ugm3']\n",
    "    elif int(home_id) in range(400,500): # USE \"scaled series - avg\"\n",
    "        PMO = pd.read_excel(rawF + PMOfile, skiprows = 2, names = ['datanum','datetime','pm_out_ugm3'])\n",
    "        PMO = PMO.dropna()\n",
    "        PMO['datetime'] = pd.to_datetime(PMO['datetime'],format = dtFormatString(PMO['datetime']))\n",
    "        PMO.index = PMO['datetime']\n",
    "        PMO = PMO['pm_out_ugm3']\n",
    "    return PMO\n",
    "\n",
    "\n",
    "def process_PMO(PMOfile,ind):\n",
    "    fname = extractName(PMOfile)    \n",
    "    home_id, instr, loc = fname.split('-',2)\n",
    "    \n",
    "    pad = 600\n",
    "    rawPMO = read_PMO(PMOfile)\n",
    "    rawPMO = rawPMO[(rawPMO.index>start-pd.Timedelta(seconds=pad)) & (rawPMO.index<stop+pd.Timedelta(seconds=pad))]\n",
    "    \n",
    "#     smPMO = rawPMO.resample('s').interpolate(method='time', axis=0, limit_area='inside')\n",
    "#     smPMO.index = smPMO.index.shift(-1, freq='30s')\n",
    "#     smPMO = smPMO.rolling('60s').mean()\n",
    "    smPMO = rawPMO.resample('s').bfill()\n",
    "    PMO = pd.merge(ind, smPMO, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    colmapper = {'pm_out_ugm3':'PMout'}\n",
    "    PMO.rename(columns = colmapper, inplace=True)\n",
    "    PMO.fillna('NA', inplace=True)\n",
    "    return PMO\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# 3.10 RAD/RDSTR - Radon: hourly data \n",
    "##############################################################################################\n",
    "def read_RAD(RADfile): # .csv, .xlsx\n",
    "    if RADfile.endswith('csv'):\n",
    "        home_id, instr, loc = re.split('-', RADfile[:-4])[0:3]\n",
    "        RAD = pd.read_csv(rawF+RADfile, names = ['datanum', 'Rn_pCiL', 'datetime', 'SN'], skiprows=1)\n",
    "    if RADfile.endswith('xlsx'):\n",
    "        home_id, instr, loc = re.split('-', RADfile[:-5])[0:3]\n",
    "        RAD = pd.read_excel(rawF+RADfile, names = ['datanum', 'Rn_pCiL', 'datetime', 'SN'], skiprows=1)\n",
    "    RAD['datetime'] = pd.to_datetime(RAD['datetime'],format = dtFormatString(RAD['datetime']))\n",
    "    serial = pd.to_numeric(RAD.SN[5])\n",
    "    RAD['Rn_pCiL'] = pd.to_numeric(RAD['Rn_pCiL'])\n",
    "    RAD = RAD.loc[RAD['Rn_pCiL']!=serial,:]\n",
    "    RAD.index = RAD['datetime']\n",
    "    RAD = RAD['Rn_pCiL']\n",
    "    return RAD\n",
    "\n",
    "def process_RAD(RADfile, ind):\n",
    "    fname = extractName(RADfile)    \n",
    "    home_id, instr, loc = re.split('-', fname)[0:3]\n",
    "    pad = 120 # minutes\n",
    "    rawRAD = read_RAD(RADfile)[start-pd.Timedelta(minutes=pad):stop+pd.Timedelta(minutes=pad)]\n",
    "    RAD = pd.merge(ind, rawRAD.resample('s').ffill(), left_index=True, right_index=True, how='left')\n",
    "\n",
    "    colmapper = {'Rn_pCiL':'Rn_{}'.format(loc)}\n",
    "    RAD.rename(columns = colmapper, inplace=True)\n",
    "    RAD.fillna('NA', inplace=True)\n",
    "    return RAD\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# 3.11 STA - Door close/open state: event record (convert) or 1min state data \n",
    "##############################################################################################\n",
    "def read_STA(STAfile): # KVZNE 'event' type record - be careful with duplicates\n",
    "    if STAfile.endswith('csv'):\n",
    "        STA = pd.read_csv(rawF+STAfile, header = None, sep=',',skiprows=findHeader(rawF+STAfile,'Date')).iloc[:,0:3]\n",
    "        STA = STA[~STA.isin(['Logged'])]\n",
    "        STA.columns = ['datanum','datetime','event/state']\n",
    "        STA['datetime'] = pd.to_datetime(STA['datetime'], format = dtFormatString(STA['datetime']))\n",
    "    elif STAfile.endswith('xlsx'):\n",
    "        STA = pd.read_excel(rawF+STAfile, usecols = [0,1,2], names = ['datanum','datetime','event/state'])\n",
    "        STA = STA[~STA.isin(['Logged'])]\n",
    "        STA['datetime'] = pd.to_datetime(STA['datetime'], format = dtFormatString(STA['datetime']))\n",
    "    else:\n",
    "        print('STA file format error')\n",
    "    if checkLogType(STA['datetime']) == 'event':\n",
    "        start = STA['datetime'].iloc[0]\n",
    "        stop = STA['datetime'].iloc[-1]\n",
    "        new_idx = pd.DataFrame(index = pd.date_range(start, stop, freq='s'))\n",
    "        STA = pd.merge(new_idx, STA, how='left', left_index=True, right_on='datetime').fillna(method='ffill')\n",
    "        STA.loc[:,'state_pct_closed'] = STA.loc[:,'event/state'] * 100 \n",
    "        print('STA logtype: event')\n",
    "    elif checkLogType(STA['datetime']) == 'state':\n",
    "        print('STA logtype: state')\n",
    "        STA['state_pct_closed'] = STA['event/state']\n",
    "    STA.index = STA['datetime']\n",
    "    STA = STA['state_pct_closed']\n",
    "    return STA\n",
    "\n",
    "\n",
    "def process_STA(STAfile, ind):\n",
    "    fname = extractName(STAfile)    \n",
    "    home_id, instr, loc = re.split('-', fname)[0:3]\n",
    "    \n",
    "    pad = 120\n",
    "    rawSTA = read_STA(STAfile)\n",
    "    rawSTA = rawSTA[(rawSTA.index>start-pd.Timedelta(seconds=pad)) & (rawSTA.index<stop+pd.Timedelta(seconds=pad))]\n",
    "    smSTA = rawSTA.resample('s').bfill()\n",
    "    STA = pd.merge(ind, smSTA, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    colmapper = {'state_pct_closed':'{}_{}'.format(instr, loc)}    \n",
    "    STA.rename(columns = colmapper, inplace=True)\n",
    "    STA.fillna('NA', inplace=True)\n",
    "    return STA\n",
    "\n",
    "##############################################################################################\n",
    "# 3.12 TRH/TRO - HOBO T/RH: 1min data (fill NA)\n",
    "##############################################################################################\n",
    "def read_TRH(TRHfile):\n",
    "    if TRHfile.endswith('csv'):\n",
    "        home_id, instr, loc = re.split('-', TRHfile[:-4])[0:3]        \n",
    "        TRH = pd.read_csv(rawF+TRHfile, usecols = [0,1,2,3],names = ['datanum','datetime','temp_degF','rh_pct'],\n",
    "                          skiprows = 2, index_col = None)\n",
    "    elif TRHfile.endswith('xlsx'):\n",
    "        home_id, instr, loc = re.split('-', TRHfile[:-5])[0:3] \n",
    "        TRH = pd.read_excel(rawF+TRHfile, usecols = [0,1,2,3],names = ['datanum','datetime','temp_degF','rh_pct'],\n",
    "                          skiprows = 2, index_col = None)\n",
    "    else:\n",
    "        print('TRH file format error')\n",
    "    TRH = TRH[~TRH.isin(['Logged'])]\n",
    "    TRH = TRH[['datetime','temp_degF','rh_pct']].dropna(axis=0,thresh=2)\n",
    "\n",
    "    TRH['datetime'] = pd.to_datetime(TRH['datetime'],format=dtFormatString(TRH['datetime']))\n",
    "    TRH['temp_degC'] = temperatureConverter(TRH['temp_degF'],'f')\n",
    "    TRH = TRH[['datetime','temp_degC','rh_pct']]          \n",
    "    if TRH['datetime'].iloc[-1] < stop:\n",
    "        newdatetime = TRH['datetime'].iloc[-1]+pd.Timedelta(minutes=1)\n",
    "        newrow = pd.Series({'datetime' : newdatetime , 'temp_degC' : None, 'rh_pct': None})\n",
    "        TRH = TRH.append(newrow,ignore_index = True)       \n",
    "        \n",
    "    TRH.index = TRH['datetime']\n",
    "    TRH = TRH[['temp_degC','rh_pct']]\n",
    "    return TRH\n",
    "\n",
    "\n",
    "def process_TRH(TRHfile, ind):\n",
    "    fname = extractName(TRHfile)    \n",
    "    home_id, instr, loc = re.split('-', fname)[0:3]    \n",
    "    pad = 60\n",
    "    rawTRH = read_TRH(TRHfile)[start-pd.Timedelta(seconds=pad):stop+pd.Timedelta(seconds=pad)]\n",
    "    TRH = pd.merge_asof(ind, rawTRH, left_index=True, right_index=True, direction='nearest')    \n",
    "    colmapper = {'temp_degC':'{}_T'.format(loc),'rh_pct':'{}_RH'.format(loc)}    \n",
    "    TRH.rename(columns = colmapper, inplace=True)\n",
    "    TRH.fillna('NA', inplace=True)\n",
    "    return TRH\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# 3.13 UFP - DISCmini: number of UltraFineParticle\n",
    "# insert last row with \"NA\" for 901\n",
    "##############################################################################################\n",
    "def read_UFP(UFPfile): # DISCmini - Betel(2units,IN1),number of particles\n",
    "    if int(home_id) in range(400,500):\n",
    "        UFP = pd.read_excel(rawF+UFPfile, header = 5,index_col = None)\n",
    "    elif int(home_id) in range(900,1000):\n",
    "        UFP = pd.read_csv(rawF+UFPfile, sep=',')\n",
    "    \n",
    "    UFP['datetime'] = pd.to_datetime(UFP['Time'], format = '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    if UFP['datetime'].iloc[-1] < stop:\n",
    "        newdatetime = UFP['datetime'].iloc[-1]+pd.Timedelta(minutes=1)\n",
    "        newrow = pd.Series({'datetime':newdatetime, 'Number':None})\n",
    "        UFP = UFP.append(newrow,ignore_index = True)\n",
    "    UFP.index = UFP['datetime']\n",
    "    UFP['ufp_count'] = pd.to_numeric(UFP['Number'])\n",
    "    UFP = UFP['ufp_count'] \n",
    "    return UFP\n",
    "\n",
    "\n",
    "def process_UFP(UFPfile,ind):\n",
    "    fname = extractName(UFPfile)    \n",
    "    home_id, instr, loc = re.split('-', fname)[0:3]   \n",
    "    \n",
    "    rawUFP = read_UFP(UFPfile)\n",
    "    \n",
    "    pad = 60 # seconds\n",
    "    rawUFP = rawUFP[(rawUFP.index>start-pd.Timedelta(seconds=pad)) & (rawUFP.index<stop+pd.Timedelta(seconds=pad))]    \n",
    "    rawUFP = rawUFP.dropna()\n",
    "    smUFP = rawUFP.resample('s').interpolate(method='time',axis=0, limit_area='inside')\n",
    "    smUFP.index = smUFP.index.shift(-1, freq='30s')\n",
    "    smUFP = smUFP.rolling('60s').mean()\n",
    "    UFP = pd.merge(df_index, smUFP, left_index=True, right_index=True, how='left')   \n",
    "   \n",
    "    colmapper = {'ufp_count':'UFP_{}'.format(loc)}\n",
    "    UFP.rename(columns=colmapper,inplace=True)\n",
    "    UFP.fillna('NA', inplace=True)\n",
    "    return UFP\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# 3.13 WTR/WX - Weather Underground Outdoor T/RH: hourly data (interpolation)\n",
    "##############################################################################################\n",
    "def read_WTR(WTRfile): # .xlsx, same as WX\n",
    "    WTR = pd.read_excel(rawF+WTRfile, header=0, index_col = False)\n",
    "    WTR['datetime'] = WTR['Date'] + pd.to_timedelta(WTR['Time'].astype(str))    \n",
    "    WTR['temp_out_degC'] = temperatureConverter(WTR['Temperature (F)'],'f')\n",
    "    WTR.rename({'Humidity (%)':'rh_out_pct'},axis = 1, inplace = True)\n",
    "    WTR.index = WTR['datetime']\n",
    "    WTR = WTR[['temp_out_degC','rh_out_pct']]\n",
    "    return WTR\n",
    "\n",
    "\n",
    "def process_WTR(WTRfile,ind):\n",
    "    fname = extractName(WTRfile)    \n",
    "    home_id, instr, loc = re.split('-', fname)[0:3]\n",
    "    \n",
    "    rawWTR = read_WTR(WTRfile)\n",
    "    WTR = pd.merge(ind, rawWTR, left_index=True, right_index=True, how='outer')\n",
    "    WTR = WTR.interpolate(method='time', axis=0, limit_area='inside')\n",
    "    WTR = pd.merge(ind, WTR, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    colmapper = {'temp_out_degC':'{}_T'.format(loc),'rh_out_pct':'{}_RH'.format(loc)}    \n",
    "    WTR.rename(columns = colmapper, inplace=True)\n",
    "    WTR.fillna('NA', inplace=True)\n",
    "    return WTR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "224-ANM-BA2.xls\n",
      "No duplicates\n",
      "1/28 files done\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "224-ANM-BA1.xls\n",
      "No duplicates\n",
      "2/28 files done\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "224-ANM-KIT.xls\n",
      "No duplicates\n",
      "3/28 files done\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "224-ANM-AS1.xls\n",
      "No duplicates\n",
      "4/28 files done\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "224-AVP-IN1.csv\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ea5a7260abda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mhome_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhome_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No duplicates'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-74f0af36e6fd>\u001b[0m in \u001b[0;36mprocessFile\u001b[0;34m(f, ind, instr, home_id, week)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocessFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhome_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweek\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minstr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'AVP'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_AVP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minstr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ANM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_ANM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-74f0af36e6fd>\u001b[0m in \u001b[0;36mprocess_AVP\u001b[0;34m(AVPfile, ind)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mhome_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m \u001b[0;31m# seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mrawAVP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_AVP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAVPfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mrawAVP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrawAVP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawAVP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrawAVP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-74f0af36e6fd>\u001b[0m in \u001b[0;36mread_AVP\u001b[0;34m(AVPfile)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mAVP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAVP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAVP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pm25_ugm3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0;36m1798.8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mAVP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAVP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAVP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pm10_ugm3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mAVP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mnewdatetime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAVP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminutes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mnewrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'datetime'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnewdatetime\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2086\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2087\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2088\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "'''\n",
    "##########################################################################################################\n",
    "4. Output: Duplicate Inspection and DataTable Writing\n",
    "##########################################################################################################\n",
    "'''\n",
    "dataTable = df_index\n",
    "countDone = 0\n",
    "for instr, fList in instrDict.items():    \n",
    "    if len(fList) > 0:\n",
    "        for f in fList:\n",
    "            print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "            home_id, instr, loc = re.split('-', f)[0:3]\n",
    "            print(f)\n",
    "            df = processFile(f, df_index, instr, home_id, week)\n",
    "            if df[df.index.duplicated(keep='first')].shape[0] == 0:\n",
    "                print('No duplicates')\n",
    "            else:\n",
    "                print ('Has duplicates XXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "            dataTable = pd.merge(dataTable, df, how='left', left_index=True, right_index=True)\n",
    "            countDone += 1\n",
    "            print(str(countDone) +'/' + str(count) + ' files done')\n",
    "            del df\n",
    "print('--------------------END--------------------')\n",
    "dataTable \n",
    "\n",
    "if week == 1 or week == None:\n",
    "    dataTable.to_csv(outF + 'DataTable{}_Week1_ON.csv'.format(home_id))\n",
    "elif week == 2:\n",
    "    dataTable.to_csv(outF + 'DataTable{}_Week2_OFF.csv'.format(home_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for summary writing use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211 week:2\n",
      "CO2_IN: 1216.7377843872928\n",
      "CO2_BR1: 1262.4652024677769\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'AVP_IN1_T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'AVP_IN1_T'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-11d13ee7aec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0mcol_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AVP_IN1_T'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0mtemp_indoor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataTable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0mc_q1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_indoor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0mc_q99\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_indoor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'AVP_IN1_T'"
     ]
    }
   ],
   "source": [
    "dataTable = dataTable.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "'''\n",
    "##########################################################################################################\n",
    "5. Output: Summary\n",
    "##########################################################################################################\n",
    "'''\n",
    "##############################################################################################\n",
    "# Load activity log\n",
    "##############################################################################################\n",
    "actF = '/Users/yuhanwang/Desktop/LBNL/BA/BA_Datatable/' # activity log folder\n",
    "if int(home_id) in range(200,400):\n",
    "    act_fname = 'Occupancy_PNNL.xlsx'\n",
    "elif int(home_id) in range(400,500):\n",
    "    act_fname = 'Occupancy_FSEC.xlsx'\n",
    "act_df = pd.read_excel(actF + act_fname)\n",
    "act_df.loc[:,'Date'] = pd.to_datetime(act_df.Date)\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# Define Support Function\n",
    "##############################################################################################\n",
    "def initiate_occupancy_df(filtered_df):\n",
    "    act_df = pd.read_excel(actF + act_fname)\n",
    "    act_df.loc[:,'Date'] = pd.to_datetime(act_df.Date)\n",
    "    list1 = pd.to_timedelta(np.arange(0,24), unit='h').tolist()\n",
    "    list2 = [list1[i] + pd.Timedelta(minutes=59, seconds=59) for i in range(len(list1))]\n",
    "    list3 = [(list1[x], list2[x]) for x in range(len(list1))]\n",
    "    hr_dict = dict(zip(act_df.columns[3:27].tolist(), list3))\n",
    "    occupy_idx = []\n",
    "    for ind, row in filtered_df.iterrows():\n",
    "        date = pd.to_datetime(row.name)\n",
    "        occupyList = row[row==True].index.tolist()\n",
    "        for i in occupyList:\n",
    "            t1 = date + hr_dict[i][0]\n",
    "            t2 = date + hr_dict[i][1]\n",
    "            occupy_idx = occupy_idx + [str(ts) for ts in pd.date_range(t1, t2, freq='min')]\n",
    "            del(t1, t2)\n",
    "        del(occupyList, date)\n",
    "    df = pd.DataFrame(index = pd.to_datetime(occupy_idx))\n",
    "    return df\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# Extract occupied slice\n",
    "##############################################################################################\n",
    "if int(home_id) in set(range(900,1000)): # KVZNE study\n",
    "        startrow = 'Field_StartDate'\n",
    "        stoprow = 'Field_EndDate'\n",
    "elif int(home_id) in set(range(200,500)): # FSEC\n",
    "        if week == 1 or week == None:\n",
    "            startrow = 'Field_StartDate_Week1'\n",
    "            stoprow = 'Field_EndDate_Week1'\n",
    "        elif week == 2:\n",
    "            startrow = 'Field_StartDate_Week2'\n",
    "            stoprow = 'Field_EndDate_Week2'\n",
    "else:\n",
    "    print(\"change home_id\")\n",
    "\n",
    "start = pd.to_datetime(ts_df.loc[startrow, int(home_id)], format = '%Y-%m-%d') + pd.Timedelta(hours=18)\n",
    "stop = pd.to_datetime(ts_df.loc[stoprow, int(home_id)], format = '%Y-%m-%d') + pd.Timedelta(hours=9)\n",
    "startday = start.strftime('%Y-%m-%d')\n",
    "stopday = stop.strftime('%Y-%m-%d')\n",
    "\n",
    "homerows = act_df.Home_ID == int(home_id)\n",
    "timecols = lambda df: df.columns[2:27]\n",
    "nightcols = act_df.columns[3:27].tolist()[0:5]\n",
    "\n",
    "allSlice = act_df.loc[homerows, timecols]\n",
    "allSlice = allSlice.set_index('Date', drop=True)\n",
    "allSlice = allSlice[startday:stopday] > 0\n",
    "nightSlice = allSlice.loc[:, nightcols]\n",
    "\n",
    "occupyAll_idx = initiate_occupancy_df(allSlice)[start:stop]\n",
    "occupyNight_idx = initiate_occupancy_df(nightSlice)[start:stop]\n",
    "\n",
    "label = home_id # caution\n",
    "results = pd.DataFrame(index=[label])\n",
    "print(home_id + ' week:' + str(week))\n",
    "\n",
    "##############################################################################################\n",
    "# a. Indoor CO2 (Living Room)\n",
    "##############################################################################################\n",
    "col_a = 'AVP_IN2_CO2'\n",
    "\n",
    "# -> (1) 24/7 + occupied\n",
    "if len(occupyAll_idx)==0:\n",
    "    occupyAll_idx = initiate_occupancy_df(allSlice.replace(allSlice,True))[start:stop]\n",
    "co2_indoor = occupyAll_idx    \n",
    "co2_indoor = pd.merge(co2_indoor, dataTable[col_a].to_frame(), left_index=True, right_index=True, how='left')\n",
    "co2_indoor_mean_occupied = co2_indoor.iloc[:,0].mean()\n",
    "print('CO2_IN: '+ str(co2_indoor_mean_occupied))\n",
    "# -> (2) 24/7\n",
    "co2_indoor_mean = dataTable[col_a].mean()\n",
    "\n",
    "##############################################################################################\n",
    "# b. Bedroom CO2 (Master Bedroom)\n",
    "##############################################################################################\n",
    "col_b = 'AVP_BR1_CO2'\n",
    "\n",
    "# Hr0->Hr4 + occupied\n",
    "if len(occupyNight_idx)==0:\n",
    "    occupyNight_idx = initiate_occupancy_df(nightSlice.where(nightSlice==True, True))[start:stop]\n",
    "    \n",
    "co2_bedroom = occupyNight_idx\n",
    "co2_bedroom = pd.merge(co2_bedroom, dataTable[col_b].to_frame(), left_index=True, right_index=True, how='left')\n",
    "co2_bedroom_mean_occupied = co2_bedroom.iloc[:,0].mean()\n",
    "\n",
    "print('CO2_BR1: '+ str(co2_bedroom_mean_occupied))\n",
    "\n",
    "##############################################################################################\n",
    "# c. Indoor Temperature (Living Room)\n",
    "##############################################################################################\n",
    "col_c = 'AVP_IN1_T'\n",
    "\n",
    "temp_indoor = dataTable[col_c]\n",
    "c_q1 = temp_indoor.quantile(0.01)\n",
    "c_q99 = temp_indoor.quantile(0.99)\n",
    "temp_indoor_mean = temp_indoor[(temp_indoor>=c_q1) & (temp_indoor<=c_q99)].mean()\n",
    "\n",
    "print('T_RangeLower (F): '+str(temperatureConverter(c_q1,'c')))\n",
    "print('T_RangeUpper (F): '+str(temperatureConverter(c_q99,'c')))\n",
    "print('T_Mean (C): '+str(temp_indoor_mean))\n",
    "\n",
    "##############################################################################################\n",
    "# d. Bedroom Temperature (Master Bedroom)\n",
    "##############################################################################################\n",
    "\n",
    "##############################################################################################\n",
    "# e. Outdoor Temperature\n",
    "##############################################################################################\n",
    "col_e = 'OUT_T'\n",
    "\n",
    "temp_outdoor = dataTable[col_e]\n",
    "e_q1 = temp_outdoor.quantile(0.01)\n",
    "e_q99 = temp_outdoor.quantile(0.99)\n",
    "temp_outdoor_mean = temp_outdoor[(temp_outdoor>=e_q1) & (temp_outdoor<=e_q99)].mean()\n",
    "print('OUT_T (C): '+str(temp_outdoor_mean))\n",
    "\n",
    "##############################################################################################\n",
    "# f. Indoor RH (Living Room)\n",
    "##############################################################################################\n",
    "col_f = 'AVP_IN1_RH'\n",
    "\n",
    "rh_indoor = dataTable[col_f]\n",
    "f_q1 = rh_indoor.quantile(0.01)\n",
    "f_q99 = rh_indoor.quantile(0.99)\n",
    "rh_indoor_mean = rh_indoor[(rh_indoor>=f_q1) & (rh_indoor<=f_q99)].mean()\n",
    "\n",
    "print('RH_RangeLower: '+str(f_q1))\n",
    "print('RH_RangeUpper: '+str(f_q99))\n",
    "print('RH_Mean: '+str(rh_indoor_mean))\n",
    "\n",
    "##############################################################################################\n",
    "# g. Bedroom RH (Master Bedroom)\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "# h. Outdoor RH\n",
    "##############################################################################################\n",
    "col_h = 'OUT_RH'\n",
    "\n",
    "rh_outdoor = dataTable[col_h]\n",
    "h_q1 = rh_outdoor.quantile(0.01)\n",
    "h_q99 = rh_outdoor.quantile(0.99)\n",
    "rh_outdoor_mean = rh_outdoor[(rh_outdoor>=h_q1) & (rh_outdoor<=h_q99)].mean()\n",
    "print('OUT_RH (C): '+str(rh_outdoor_mean))\n",
    "\n",
    "##############################################################################################\n",
    "# g. Radon\n",
    "##############################################################################################\n",
    "col_g = 'Rn_BS1'\n",
    "\n",
    "radon = dataTable[col_g]\n",
    "g_q1 = radon.quantile(0.01)\n",
    "g_q99 = radon.quantile(0.99)\n",
    "radon_mean = radon[(radon>=g_q1) & (radon<=g_q99)].mean()\n",
    "print('Radon: ' +str(radon_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
